{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\n",
    "    'finger-1', 'finger-2', 'finger-3', 'finger-4', 'finger-5',\n",
    "    'finger-6', 'finger-7', 'finger-8', 'finger-9', 'finger-10',\n",
    "    'finger-11', 'finger-12', 'finger-13', 'finger-14', 'finger-15',\n",
    "    'finger-16', 'finger-17', 'finger-18', 'finger-19', 'Trapezium',\n",
    "    'Trapezoid', 'Capitate', 'Hamate', 'Scaphoid', 'Lunate',\n",
    "    'Triquetrum', 'Pisiform', 'Radius', 'Ulna',\n",
    "]\n",
    "\n",
    "CLASS2IND = {v: i for i, v in enumerate(CLASSES)}\n",
    "IND2CLASS = {v: k for k, v in CLASS2IND.items()}\n",
    "\n",
    "\n",
    "PALETTE = [\n",
    "    (220, 20, 60), (119, 11, 32), (0, 0, 142), (0, 0, 230), (106, 0, 228),\n",
    "    (0, 60, 100), (0, 80, 100), (0, 0, 70), (0, 0, 192), (250, 170, 30),\n",
    "    (100, 170, 30), (220, 220, 0), (175, 116, 175), (250, 0, 30), (165, 42, 42),\n",
    "    (255, 77, 255), (0, 226, 252), (182, 182, 255), (0, 82, 0), (120, 166, 157),\n",
    "    (110, 76, 0), (174, 57, 255), (199, 100, 0), (72, 0, 118), (255, 179, 240),\n",
    "    (0, 125, 92), (209, 0, 151), (188, 208, 182), (0, 220, 176),\n",
    "]\n",
    "\n",
    "csv_path=\"../../outputs/ensemble_soft_output.csv\"\n",
    "output_dir=\"./segmentation_visualizations\"\n",
    "base_image_dir=\"../../data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_decode(mask_rle: str, shape: tuple[int, int]) -> np.ndarray:\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0::2], s[1::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "def label2rgb(label: np.ndarray, height: int, width: int) -> np.ndarray:\n",
    "    image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    for i, class_mask in enumerate(label):\n",
    "        image[class_mask == 1] = PALETTE[i]\n",
    "    return image\n",
    "\n",
    "def load_image_paths():\n",
    "    image_paths = {}\n",
    "    for root, _, files in os.walk(base_image_dir):\n",
    "        for f in files:\n",
    "            if f.endswith('.png') or f.endswith('.jpg'):\n",
    "                image_paths[f] = os.path.join(root, f)\n",
    "    return image_paths\n",
    "\n",
    "def visualize_all_segmentations():\n",
    "    df = pd.read_csv(csv_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    grouped = df.groupby(\"image_name\")    \n",
    "    image_paths = load_image_paths()  \n",
    "    \n",
    "    for image_name, group in grouped:\n",
    "        image_path = image_paths.get(image_name) \n",
    "        if image_path is None:\n",
    "            print(f\"Image not found: {image_name}\")\n",
    "            continue\n",
    "        \n",
    "        image = np.array(Image.open(image_path).convert('RGB'))\n",
    "        height, width = image.shape[:2]\n",
    "        mask = np.zeros((len(PALETTE), height, width), dtype=np.uint8)\n",
    "        \n",
    "        for _, row in group.iterrows():\n",
    "            class_name, rle = row['class'], row['rle']\n",
    "            if pd.isna(rle):\n",
    "                continue\n",
    "            decoded_mask = rle_decode(rle, (height, width))\n",
    "            mask[CLASS2IND[class_name]] = decoded_mask\n",
    "        \n",
    "        color_mask = label2rgb(mask, height, width)\n",
    "        blended = cv2.addWeighted(image, 0.5, color_mask, 0.5, 0)\n",
    "        \n",
    "        output_path = os.path.join(output_dir, f\"{image_name.split('.')[0]}_seg.png\")\n",
    "        plt.imsave(output_path, blended)\n",
    "        print(f\"Saved segmentation visualization for {image_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_all_segmentations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_segmentation_pair(image_path: str, segmentation_path: str) -> None:\n",
    "    # 원본 이미지 로드\n",
    "    image = np.array(Image.open(image_path).convert('RGB'))\n",
    "    # 세그멘테이션 이미지 로드\n",
    "    segmentation = np.array(Image.open(segmentation_path).convert('RGB'))\n",
    "    \n",
    "    # 플롯 설정\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(segmentation)\n",
    "    axes[1].set_title(\"Segmentation\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_all_images_with_segmentations():\n",
    "    segmentation_files = [f for f in os.listdir(output_dir) if f.endswith('_seg.png')]\n",
    "    \n",
    "    for i, seg_file in enumerate(segmentation_files[:5]): \n",
    "        image_name = seg_file.replace('_seg.png', '.png')  \n",
    "        image_path = image_paths.get(image_name)  \n",
    "        segmentation_path = os.path.join(output_dir, seg_file)\n",
    "        \n",
    "        if image_path is None:\n",
    "            print(f\"Original image not found: {image_name}\")\n",
    "            continue\n",
    "        \n",
    "        display_segmentation_pair(image_path, segmentation_path)\n",
    "\n",
    "\n",
    "image_paths = load_image_paths()  \n",
    "\n",
    "visualize_all_images_with_segmentations()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 이미지 중에서 잘 분류하지 못하는 것들은 무엇이 있을까?\n",
    "\n",
    "train dataset으로 결과를 확인해보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"../../data/train/DCM\"\n",
    "label_path = \"../../data/train/outputs_json\"\n",
    "top_k = 5\n",
    "config_path = \"../../outputs/dev_smp_unet_kh\"\n",
    "pth_path = \"../../outputs/dev_smp_unet_kh/smp_unet_best_model.pth\"\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/data/ephemeral/home/kwak/level2-cv-semanticsegmentation-cv-18-lv3\")\n",
    "from src.models.model_utils import *\n",
    "from src.datasets.dataset import XRayDataset\n",
    "from src.utils.augmentation import get_transform\n",
    "from src.utils.metrics import get_metric_function\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from src.utils.rle_convert import encode_mask_to_rle\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "def get_config(config_folder):\n",
    "    config = {}\n",
    "\n",
    "    config_folder = os.path.join(config_folder,'*.yaml')\n",
    "    \n",
    "    config_files = glob.glob(config_folder)\n",
    "    \n",
    "    for file in config_files:\n",
    "        with open(file, 'r') as f:\n",
    "            config.update(yaml.safe_load(f))\n",
    "    \n",
    "    if config['device'] == 'cuda' and not torch.cuda.is_available():\n",
    "        print('using cpu now...')\n",
    "        config['device'] = 'cpu'\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋 받아오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(config_path)\n",
    "\n",
    "classes = config['classes']\n",
    "CLASS2IND = {v: i for i, v in enumerate(classes)}\n",
    "IND2CLASS = {v: k for k, v in CLASS2IND.items()}\n",
    "\n",
    "transform = get_transform(config['data'], is_train=False)\n",
    "\n",
    "_dataset = XRayDataset(\n",
    "    image_root=img_path,\n",
    "    label_root=label_path,\n",
    "    classes=config['classes'],\n",
    "    mode='val',\n",
    "    transforms=transform\n",
    ")\n",
    "\n",
    "_dataloader = DataLoader(\n",
    "    _dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(config['device'])\n",
    "model = get_model(config['model'], config['classes']).to(device)\n",
    "\n",
    "checkpoint = torch.load(pth_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "criterion = get_criterion(config['train']['criterion']['name'])\n",
    "metric_fn = get_metric_function(config['train']['metric']['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation 수행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculating: 100%|██████████| 20/20 [09:57<00:00, 29.88s/it]\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "dice_list = []\n",
    "\n",
    "rles = []\n",
    "gt_rles = []\n",
    "filename_and_class = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(_dataloader, desc=\"calculating\")):\n",
    "        inputs, masks = batch\n",
    "        inputs, masks = inputs.to(device), masks.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        if isinstance(outputs, tuple):\n",
    "                logits, logits1, logits2 = outputs\n",
    "                use_multiple_outputs = True\n",
    "        else: \n",
    "            logits = outputs['out'] if isinstance(outputs, dict) and 'out' in outputs else outputs\n",
    "            use_multiple_outputs = False\n",
    "\n",
    "        logits_h, logits_w = logits.size(-2), logits.size(-1)\n",
    "        labels_h, labels_w = masks.size(-2), masks.size(-1)\n",
    "\n",
    "        #출력과 레이블의 크기가 다른 경우 출력 텐서를 레이블의 크기로 보간\n",
    "        if logits_h != labels_h or logits_w != labels_w:\n",
    "            logits = F.interpolate(logits, size=(labels_h, labels_w), mode=\"bilinear\", align_corners=False)\n",
    "            if use_multiple_outputs:\n",
    "                logits1 = F.interpolate(logits1, size=(labels_h, labels_w), mode=\"bilinear\", align_corners=False)\n",
    "                logits2 = F.interpolate(logits2, size=(labels_h, labels_w), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        probs = torch.sigmoid(logits)\n",
    "        # threshold 추가해서 기준 치 이상만 label로 분류 \n",
    "        # outputs = (probs > threshold).detach().cpu()\n",
    "        # masks = masks.detach().cpu()\n",
    "        outputs = (probs > threshold)\n",
    "        dice = metric_fn.calculate(outputs, masks).detach().cpu()\n",
    "        \n",
    "        dice_list.append(dice)\n",
    "        \n",
    "        # pd에 저장하기 위함\n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "        masks = masks.detach().cpu().numpy()\n",
    "        \n",
    "        for idx, (output, mask) in enumerate(zip(outputs, masks)):\n",
    "            file_path, _ = _dataset.get_filepath(idx*i)\n",
    "            \n",
    "            for c, (seg_m, gt_m) in enumerate(zip(output, mask)):\n",
    "                rle = encode_mask_to_rle(seg_m)\n",
    "                gt_rle = encode_mask_to_rle(gt_m)\n",
    "                rles.append(rle)\n",
    "                gt_rles.append(gt_rle)\n",
    "                filename_and_class.append(f\"{IND2CLASS[c]}:{file_path}\")\n",
    "\n",
    "classes, file_path = zip(*[x.split(\":\") for x in filename_and_class])\n",
    "df_pred = pd.DataFrame({\n",
    "    \"file_path\": file_path,\n",
    "    \"class\": classes,\n",
    "    \"rle\": rles,\n",
    "    \"gt_rle\": gt_rles,\n",
    "})\n",
    "\n",
    "# dice_list = sorted(dice_list, key=lambda x: (-x, x))\n",
    "\n",
    "dices = torch.cat(dice_list, 0)\n",
    "\n",
    "dices_per_img = torch.mean(dices, 1)\n",
    "\n",
    "file_path, _ = _dataset.get_all_path()\n",
    "df_dice = pd.DataFrame(dices, columns=[IND2CLASS[i] for i in range(29)])\n",
    "df_dice.insert(0, \"file_path\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_csv_path = os.path.join(config_path, \"vis_csv\")\n",
    "df_pred_path = os.path.join(vis_csv_path, \"pred.csv\")\n",
    "df_dice_path = os.path.join(vis_csv_path, \"dice.csv\")\n",
    "\n",
    "os.makedirs(vis_csv_path, exist_ok=True)\n",
    "\n",
    "df_pred.to_csv(df_pred_path, index=False)\n",
    "df_dice.to_csv(df_dice_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 중 dice score 값이 제일 낮은 결과값들 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = torch.topk(dices_per_img, k=top_k, largest=False)\n",
    "values, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = torch.argsort(dices_per_img)\n",
    "indices_ = sorted_indices[:10]\n",
    "\n",
    "l_v = dices_per_img[indices_]\n",
    "l_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0067)\n",
      "('ID070/image1661736042863.png', 'ID070/image1661736042863.json')\n",
      "tensor(0.0068)\n",
      "('ID444/image1666144319702.png', 'ID444/image1666144319702.json')\n",
      "tensor(0.0069)\n",
      "('ID506/image1666747111320.png', 'ID506/image1666747111320.json')\n",
      "tensor(0.0070)\n",
      "('ID480/image1666661091576.png', 'ID480/image1666661091576.json')\n",
      "tensor(0.0070)\n",
      "('ID004/image1661144724044.png', 'ID004/image1661144724044.json')\n"
     ]
    }
   ],
   "source": [
    "for v, i in zip(values, indices):\n",
    "    print(v)\n",
    "    print(_dataset.get_filepath(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 23, 119, 145, 135,   3, 131,  27, 144, 147,  45, 155,  97,  29,  25,\n",
       "        115,  11,  13,   1, 149, 153, 151,  15,  57,  83, 137,  75, 154,  51,\n",
       "         91,  49,  21,  17,  71,  56, 123,  73,  33, 157,  39, 129, 103,   5,\n",
       "         95,  44,  31,   2, 139,  54, 133,  61, 152,  41, 159,  22,  19,  89,\n",
       "         35, 111,  10,  55, 121, 117,  59,  37, 107, 118,  87, 130,  99,  43,\n",
       "        150,   0,  96,  24, 141, 125, 146, 134,  12,  53, 105,  26, 114,   7,\n",
       "         74,   9, 148,  47,  32,  63, 127,  50,  30,  77,  90, 138,  81,  60,\n",
       "         67,  34,  42, 143,  79,  69, 136, 101,  58,  28, 128, 116,  62,  72,\n",
       "        158,   6, 120,  85,  48,  94,  14,  65,  20, 142,  16,  52, 109,  86,\n",
       "        102,  82,  40, 122, 126, 140,  70,   4, 106, 132,   8,  38, 110,  80,\n",
       "        124,  66,  68,  88,  46, 108, 100, 113, 156,  64,  18,  36,  93,  76,\n",
       "         84, 112,  78,  98,  92, 104])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
