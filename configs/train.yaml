train:
  batch_size: 4
  num_epochs: 100
  n_splits: 5
  threshold: 0.5
   
  metric: 
    name: "dice_coef"
  # accuracy / f1 / precision / recall / mse / dice_coef

  # 이미 학습한 모델의 ckpt를 가지고 재학습 할 것인지 
  resume: False
  #어떤 ckpt로부터 재학습할 것인지
  ckpt_path : ""

  criterion: 
    name: "StructureLoss"
  # CrossEntropyLoss / BCEWithLogitsLoss / bce+dice / StructureLoss(for SAM2-UNET)

  optimizer:
    name: "Adam"
    # Adam / SGD
    config:
      weight_decay: 1.e-6
      # momentum: 0.9
      lr: 1.e-4

  lr_scheduler:
    name: "ReduceLROnPlateau"
    monitor: "metric"
    config:
      factor: 0.1
      patience: 15
      min_lr: 1.e-6
      verbose: True

  early_stopping:
    patience: 5
    min_delta: 0.001
    evidence: 'loss'
    monitor: 'metric'

model:
  name: "fcn_50"

  # fcn_50 /
  config:
    pretrained: True
  